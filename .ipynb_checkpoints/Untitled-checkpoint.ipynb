{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanwenyu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "from glob import glob\n",
    "import os\n",
    "import keras\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, stride, padding=\"SAME\"):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(x, k_size, stride, padding=\"SAME\"):\n",
    "    # use avg pooling instead, as described in the paper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k_size, k_size, 1], \n",
    "            strides=[1, stride, stride, 1], padding=padding)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_modify(x):\n",
    "    data = np.load('/Users/hanwenyu/Desktop/EECS349/vgg16_weights.npz')\n",
    "    key=data.keys()\n",
    "    Key=np.sort(key)\n",
    "    ### load weights\n",
    "    W_conv1_1 = tf.get_variable(name=\"W_conv1_1\",initializer=data[Key[0]])\n",
    "    W_conv1_2 = tf.get_variable(name=\"W_conv1_2\",initializer=data[Key[2]])\n",
    "    W_conv2_1 = tf.get_variable(name=\"W_conv2_1\",initializer=data[Key[4]])\n",
    "    W_conv2_2 = tf.get_variable(name=\"W_conv2_2\",initializer=data[Key[6]])\n",
    "    W_conv3_1 = tf.get_variable(name=\"W_conv3_1\",initializer=data[Key[8]])\n",
    "    W_conv3_2 = tf.get_variable(name=\"W_conv3_2\",initializer=data[Key[10]])\n",
    "    W_conv3_3 = tf.get_variable(name=\"W_conv3_3\",initializer=data[Key[12]])\n",
    "    W_conv4_1 = tf.get_variable(name=\"W_conv4_1\",initializer=data[Key[14]])\n",
    "    W_conv4_2 = tf.get_variable(name=\"W_conv4_2\",initializer=data[Key[16]])\n",
    "    W_conv4_3 = tf.get_variable(name=\"W_conv4_3\",initializer=data[Key[18]])\n",
    "    W_conv5_1 = tf.get_variable(name=\"W_conv5_1\",initializer=data[Key[20]])\n",
    "    W_conv5_2 = tf.get_variable(name=\"W_conv5_2\",initializer=data[Key[22]])\n",
    "    W_conv5_3 = tf.get_variable(name=\"W_conv5_3\",initializer=data[Key[24]])\n",
    "    W_fc6     = tf.get_variable(name=\"W_fc6\",initializer=data[Key[26]])   \n",
    "    W_fc7     = tf.get_variable(name=\"W_fc7\",initializer=data[Key[28]]) \n",
    "    W_fc8     = tf.get_variable(name=\"W_fc8\",initializer=data[Key[30]]) \n",
    "    # load bias \n",
    "    \n",
    "    b_conv1_1 = tf.get_variable(name=\"b_conv1_1\",initializer=data[Key[1]])\n",
    "    b_conv1_2 = tf.get_variable(name=\"b_conv1_2\",initializer=data[Key[3]])\n",
    "    b_conv2_1 = tf.get_variable(name=\"b_conv2_1\",initializer=data[Key[5]])\n",
    "    b_conv2_2 = tf.get_variable(name=\"b_conv2_2\",initializer=data[Key[7]])\n",
    "    b_conv3_1 = tf.get_variable(name=\"b_conv3_1\",initializer=data[Key[9]])\n",
    "    b_conv3_2 = tf.get_variable(name=\"b_conv3_2\",initializer=data[Key[11]])\n",
    "    b_conv3_3 = tf.get_variable(name=\"b_conv3_3\",initializer=data[Key[13]])\n",
    "    b_conv4_1 = tf.get_variable(name=\"b_conv4_1\",initializer=data[Key[15]])\n",
    "    b_conv4_2 = tf.get_variable(name=\"b_conv4_2\",initializer=data[Key[17]])\n",
    "    b_conv4_3 = tf.get_variable(name=\"b_conv4_3\",initializer=data[Key[19]])\n",
    "    b_conv5_1 = tf.get_variable(name=\"b_conv5_1\",initializer=data[Key[21]])\n",
    "    b_conv5_2 = tf.get_variable(name=\"b_conv5_2\",initializer=data[Key[23]])\n",
    "    b_conv5_3 = tf.get_variable(name=\"b_conv5_3\",initializer=data[Key[25]])\n",
    "    b_fc6     = tf.get_variable(name=\"b_fc6\",initializer=data[Key[27]])\n",
    "    b_fc7     = tf.get_variable(name=\"b_fc7\",initializer=data[Key[29]])\n",
    "    b_fc8     = tf.get_variable(name=\"b_fc8\",initializer=data[Key[31]])\n",
    "    \n",
    "    ## build model \n",
    "    ### Block 1 \n",
    "    conv1_1= conv2d(x, W_conv1_1, stride=1, padding='SAME')\n",
    "    conv1_1= tf.nn.bias_add(conv1_1, b_conv1_1)\n",
    "    conv1_1=tf.nn.relu(conv1_1)\n",
    "    \n",
    "    conv1_2=conv2d(conv1_1,W_conv1_2,stride=1, padding='SAME')\n",
    "    conv1_2=tf.nn.bias_add(conv1_2, b_conv1_2)\n",
    "    conv1_2=tf.nn.relu(conv1_2)\n",
    "    conv1_2=max_pool(conv1_2,k_size=2,stride=2,padding='SAME')  \n",
    "    \n",
    "    #### Block 2\n",
    "    \n",
    "    conv2_1= conv2d(conv1_2, W_conv2_1, stride=1, padding='SAME')\n",
    "    conv2_1= tf.nn.bias_add(conv2_1, b_conv2_1)\n",
    "    conv2_1=tf.nn.relu(conv2_1)\n",
    "    \n",
    "    conv2_2=conv2d(conv2_1,W_conv2_2,stride=1, padding='SAME')\n",
    "    conv2_2=tf.nn.bias_add(conv2_2, b_conv2_2)\n",
    "    conv2_2=tf.nn.relu(conv2_2)\n",
    "    conv2_2=max_pool(conv2_2,k_size=2,stride=2,padding='SAME')  \n",
    "    \n",
    "    #### Block 3\n",
    "    \n",
    "    conv3_1= conv2d(conv2_2, W_conv3_1, stride=1, padding='SAME')\n",
    "    conv3_1= tf.nn.bias_add(conv3_1, b_conv3_1)\n",
    "    conv3_1=tf.nn.relu(conv3_1)\n",
    "    \n",
    "    conv3_2= conv2d(conv3_1, W_conv3_2, stride=1, padding='SAME')\n",
    "    conv3_2= tf.nn.bias_add(conv3_2, b_conv3_2)\n",
    "    conv3_2=tf.nn.relu(conv3_2)\n",
    "    \n",
    "    conv3_3=conv2d(conv3_2,W_conv3_3,stride=1, padding='SAME')\n",
    "    conv3_3=tf.nn.bias_add(conv3_3, b_conv3_3)\n",
    "    conv3_3=tf.nn.relu(conv3_3)\n",
    "    conv3_3=max_pool(conv3_3,k_size=2,stride=2,padding='SAME')  \n",
    "    \n",
    "    \n",
    "    ### Block 4\n",
    "    \n",
    "    conv4_1= conv2d(conv3_3, W_conv4_1, stride=1, padding='SAME')\n",
    "    conv4_1= tf.nn.bias_add(conv4_1, b_conv4_1)\n",
    "    conv4_1=tf.nn.relu(conv4_1)\n",
    "    \n",
    "    conv4_2= conv2d(conv4_1, W_conv4_2, stride=1, padding='SAME')\n",
    "    conv4_2= tf.nn.bias_add(conv4_2, b_conv4_2)\n",
    "    conv4_2=tf.nn.relu(conv4_2)\n",
    "    \n",
    "    conv4_3=conv2d(conv4_2,W_conv4_3,stride=1, padding='SAME')\n",
    "    conv4_3=tf.nn.bias_add(conv4_3, b_conv4_3)\n",
    "    conv4_3=tf.nn.relu(conv4_3)\n",
    "    conv4_3=max_pool(conv4_3,k_size=2,stride=2,padding='SAME')  \n",
    "    \n",
    "    #### Block 5\n",
    "    \n",
    "    conv5_1= conv2d(conv4_3, W_conv5_1, stride=1, padding='SAME')\n",
    "    conv5_1= tf.nn.bias_add(conv5_1, b_conv5_1)\n",
    "    conv5_1=tf.nn.relu(conv5_1)\n",
    "    \n",
    "    conv5_2= conv2d(conv5_1, W_conv5_2, stride=1, padding='SAME')\n",
    "    conv5_2= tf.nn.bias_add(conv5_2, b_conv5_2)\n",
    "    conv5_2=tf.nn.relu(conv5_2)\n",
    "    \n",
    "    conv5_3=conv2d(conv5_2,W_conv5_3,stride=1, padding='SAME')\n",
    "    conv5_3=tf.nn.bias_add(conv5_3, b_conv5_3)\n",
    "    conv5_3=tf.nn.relu(conv5_3)\n",
    "    conv5_3=max_pool(conv5_3,k_size=2,stride=2,padding='SAME')  \n",
    "    \n",
    "    conv5_3=tf.reshape(conv5_3, [-1, 7 * 7 * 512])\n",
    "    \n",
    "    #### Block 6 \n",
    "    \n",
    "    fc6=tf.matmul(conv5_3,W_fc6)+b_fc6\n",
    "    \n",
    "    fc7=tf.matmul(fc6,W_fc7)+b_fc7\n",
    "    \n",
    "    fc8=tf.matmul(fc7,W_fc8)+b_fc8\n",
    "    \n",
    "    logits_1 = tf.nn.sigmoid(tf.layers.dense(inputs=fc8, units=5))\n",
    "    logits_2 = tf.nn.sigmoid(tf.layers.dense(inputs=fc8, units=5))\n",
    "    logits_3 = tf.nn.sigmoid(tf.layers.dense(inputs=fc8, units=6))\n",
    "    logits_4 = tf.nn.sigmoid(tf.layers.dense(inputs=fc8, units=9))\n",
    "    logits_5 = tf.nn.sigmoid(tf.layers.dense(inputs=fc8, units=6))\n",
    "    logits_6 = tf.nn.sigmoid(tf.layers.dense(inputs=fc8, units=5))\n",
    "    logits_7 = tf.nn.sigmoid(tf.layers.dense(inputs=fc8, units=8))\n",
    "    logits_8 = tf.nn.sigmoid(tf.layers.dense(inputs=fc8, units=10))\n",
    "    \n",
    "    return logits_1, logits_2, logits_3, logits_4, logits_5, logits_6, logits_7, logits_8\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None,224,224,3) , name='x')\n",
    "y_1=tf.placeholder(tf.float32, shape=(None,5) , name='y_1')\n",
    "y_2=tf.placeholder(tf.float32, shape=(None,5) , name='y_2')\n",
    "y_3=tf.placeholder(tf.float32, shape=(None,6) , name='y_3')\n",
    "y_4=tf.placeholder(tf.float32, shape=(None,9) , name='y_4')\n",
    "y_5=tf.placeholder(tf.float32, shape=(None,6) , name='y_5')\n",
    "y_6=tf.placeholder(tf.float32, shape=(None,5) , name='y_6')\n",
    "y_7=tf.placeholder(tf.float32, shape=(None,8) , name='y_7')\n",
    "y_8=tf.placeholder(tf.float32, shape=(None,10) , name='y_8')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lr=0.0005\n",
    "batch_size=10\n",
    "output_1,output_2,output_3,output_4,output_5,output_6,output_7,output_8=VGG_modify(x)\n",
    "Weight_array=tf.placeholder(tf.float32, shape=(8,batch_size) , name='Weight_array')\n",
    "\n",
    "loss_1=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_1,\n",
    "                                             labels=y_1),axis=1)\n",
    "\n",
    "loss_2=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_2,\n",
    "                                             labels=y_2),axis=1)\n",
    "loss_3=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_3,\n",
    "                                             labels=y_3),axis=1)\n",
    "loss_4=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_4,\n",
    "                                             labels=y_4),axis=1)\n",
    "loss_5=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_5,\n",
    "                                             labels=y_5),axis=1)\n",
    "loss_6=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_6,\n",
    "                                             labels=y_6),axis=1)\n",
    "loss_7=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_7,\n",
    "                                             labels=y_7),axis=1)\n",
    "loss_8=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_8,\n",
    "                                             labels=y_8),axis=1)\n",
    "\n",
    "loss_array= tf.stack([loss_1,loss_2,loss_3,loss_4,loss_5,loss_6,loss_7,loss_8]) \n",
    "\n",
    "loss_array=tf.transpose(loss_array)\n",
    "\n",
    "loss=tf.matmul(loss_array,Weight_array)\n",
    "\n",
    "loss= tf.diag_part(loss)\n",
    "\n",
    "loss= tf.math.reduce_sum(loss)\n",
    "\n",
    "\n",
    "optimize=tf.train.AdamOptimizer(lr, beta1=0.5, epsilon=1e-7).minimize(loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/hanwenyu/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    \n",
    "    input_x=np.random.uniform(-1, 1, (batch_size,224,224,3))\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        optimal=sess.run(optimize,{x:input_x})\n",
    "        save_path = saver.save(sess, \"/Users/hanwenyu/Desktop/EECS349/model.ckpt\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
